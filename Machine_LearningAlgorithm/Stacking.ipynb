{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking \n",
    "- 여러 학습기에서 예측한 예측값으로 다시 학습 데이터를 만들어 일반화된 최종 모델을 구성하는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd \n",
    "data = pd.read_csv(\"../Data/breast-cancer-wisconsin.csv\")\n",
    "X = data.iloc[:, 1:10]\n",
    "y = data[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 410)\n",
    "\n",
    "from sklearn.preprocessing import * \n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train)\n",
    "X_scaled_train = minmax.transform(X_train)\n",
    "X_scaled_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import * \n",
    "import sklearn\n",
    "from sklearn.svm import * \n",
    "from sklearn.linear_model import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaBoostClassifier',\n",
       " 'AdaBoostRegressor',\n",
       " 'BaggingClassifier',\n",
       " 'BaggingRegressor',\n",
       " 'BaseEnsemble',\n",
       " 'ExtraTreesClassifier',\n",
       " 'ExtraTreesRegressor',\n",
       " 'GradientBoostingClassifier',\n",
       " 'GradientBoostingRegressor',\n",
       " 'HistGradientBoostingClassifier',\n",
       " 'HistGradientBoostingRegressor',\n",
       " 'IsolationForest',\n",
       " 'RandomForestClassifier',\n",
       " 'RandomForestRegressor',\n",
       " 'RandomTreesEmbedding',\n",
       " 'StackingClassifier',\n",
       " 'StackingRegressor',\n",
       " 'VotingClassifier',\n",
       " 'VotingRegressor',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_bagging',\n",
       " '_base',\n",
       " '_forest',\n",
       " '_gb',\n",
       " '_gb_losses',\n",
       " '_gradient_boosting',\n",
       " '_hist_gradient_boosting',\n",
       " '_iforest',\n",
       " '_stacking',\n",
       " '_voting',\n",
       " '_weight_boosting']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StackingClassifier in module sklearn.ensemble._stacking:\n",
      "\n",
      "class StackingClassifier(sklearn.base.ClassifierMixin, _BaseStacking)\n",
      " |  StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      " |  \n",
      " |  Stack of estimators with a final classifier.\n",
      " |  \n",
      " |  Stacked generalization consists in stacking the output of individual\n",
      " |  estimator and use a classifier to compute the final prediction. Stacking\n",
      " |  allows to use the strength of each individual estimator by using their\n",
      " |  output as input of a final estimator.\n",
      " |  \n",
      " |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      " |  is trained using cross-validated predictions of the base estimators using\n",
      " |  `cross_val_predict`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <stacking>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.22\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimators : list of (str, estimator)\n",
      " |      Base estimators which will be stacked together. Each element of the\n",
      " |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      " |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      " |  \n",
      " |  final_estimator : estimator, default=None\n",
      " |      A classifier which will be used to combine the base estimators.\n",
      " |      The default classifier is a\n",
      " |      :class:`~sklearn.linear_model.LogisticRegression`.\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy used in\n",
      " |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      " |      cv are:\n",
      " |  \n",
      " |      * None, to use the default 5-fold cross validation,\n",
      " |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      " |      * An object to be used as a cross-validation generator,\n",
      " |      * An iterable yielding train, test splits.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and y is\n",
      " |      either binary or multiclass,\n",
      " |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      " |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      " |      These splitters are instantiated with `shuffle=False` so the splits\n",
      " |      will be the same across calls.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. note::\n",
      " |         A larger number of split will provide no benefits if the number\n",
      " |         of training samples is large enough. Indeed, the training time\n",
      " |         will increase. ``cv`` is not used for model evaluation but for\n",
      " |         prediction.\n",
      " |  \n",
      " |  stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n",
      " |      Methods called for each base estimator. It can be:\n",
      " |  \n",
      " |      * if 'auto', it will try to invoke, for each estimator,\n",
      " |        `'predict_proba'`, `'decision_function'` or `'predict'` in that\n",
      " |        order.\n",
      " |      * otherwise, one of `'predict_proba'`, `'decision_function'` or\n",
      " |        `'predict'`. If the method is not implemented by the estimator, it\n",
      " |        will raise an error.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel all `estimators` `fit`.\n",
      " |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      " |      using all processors. See Glossary for more details.\n",
      " |  \n",
      " |  passthrough : bool, default=False\n",
      " |      When False, only the predictions of estimators will be used as\n",
      " |      training data for `final_estimator`. When True, the\n",
      " |      `final_estimator` is trained on the predictions as well as the\n",
      " |      original training data.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity level.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      Class labels.\n",
      " |  \n",
      " |  estimators_ : list of estimators\n",
      " |      The elements of the estimators parameter, having been fitted on the\n",
      " |      training data. If an estimator has been set to `'drop'`, it\n",
      " |      will not appear in `estimators_`.\n",
      " |  \n",
      " |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      " |      Attribute to access any fitted sub-estimators by name.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying classifier exposes such an attribute when fit.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Only defined if the\n",
      " |      underlying estimators expose such an attribute when fit.\n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  final_estimator_ : estimator\n",
      " |      The classifier which predicts given the output of `estimators_`.\n",
      " |  \n",
      " |  stack_method_ : list of str\n",
      " |      The method used by each base estimator.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  StackingRegressor : Stack of estimators with a final regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  When `predict_proba` is used by each estimator (i.e. most of the time for\n",
      " |  `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n",
      " |  The first column predicted by each estimator will be dropped in the case\n",
      " |  of a binary classification problem. Indeed, both feature will be perfectly\n",
      " |  collinear.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      " |     (1992): 241-259.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.ensemble import StackingClassifier\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> estimators = [\n",
      " |  ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
      " |  ...     ('svr', make_pipeline(StandardScaler(),\n",
      " |  ...                           LinearSVC(random_state=42)))\n",
      " |  ... ]\n",
      " |  >>> clf = StackingClassifier(\n",
      " |  ...     estimators=estimators, final_estimator=LogisticRegression()\n",
      " |  ... )\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      " |  ...     X, y, stratify=y, random_state=42\n",
      " |  ... )\n",
      " |  >>> clf.fit(X_train, y_train).score(X_test, y_test)\n",
      " |  0.9...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StackingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      _BaseStacking\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Decision function for samples in `X` using the final estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      decisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          The decision function computed the final estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the estimators.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |          Note that this is supported only if all underlying estimators\n",
      " |          support sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns a fitted instance of estimator.\n",
      " |  \n",
      " |  predict(self, X, **predict_params)\n",
      " |      Predict target for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      **predict_params : dict of str -> obj\n",
      " |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      " |          that this may be used to return uncertainties from some estimators\n",
      " |          with `return_std` or `return_cov`. Be aware that it will only\n",
      " |          accounts for uncertainty in the final estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      " |          Predicted targets.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for `X` using the final estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      probabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n",
      " |          The class probabilities of the input samples.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Return class labels or probabilities for X for each estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n",
      " |          Prediction outputs for each estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _BaseStacking:\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      " |      and returns a transformed version of `X`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input samples.\n",
      " |      \n",
      " |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      " |          Target values (None for unsupervised transformations).\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Returns the parameters given in the constructor as well as the\n",
      " |      estimators contained within the `estimators` parameter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          Setting it to True gets the various estimators and the parameters\n",
      " |          of the estimators as well.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter and estimator names mapped to their values or parameter\n",
      " |          names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of an estimator from the ensemble.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      " |      can directly set the parameters of the estimators contained in\n",
      " |      `estimators`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : keyword arguments\n",
      " |          Specific parameters using e.g.\n",
      " |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      " |          parameters of the estimator, the individual estimator of the\n",
      " |          estimators can also be set, or can be removed by setting them to\n",
      " |          'drop'.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      " |  \n",
      " |  named_estimators\n",
      " |      Dictionary to access any fitted sub-estimators by name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~sklearn.utils.Bunch`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      " |  \n",
      " |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StackingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [(\"rf\", RandomForestClassifier(n_estimators=10, random_state = 410)),\n",
    "              (\"svr\", SVC(random_state=410))]\n",
    "model = StackingClassifier(estimators= estimators, final_estimator=LogisticRegression())\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[325   8]\n",
      " [  4 175]] \n",
      "\n",
      "[[107   4]\n",
      " [  0  60]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import * \n",
    "con_train = confusion_matrix(y_train, pred_train)\n",
    "print(con_train, \"\\n\")\n",
    "\n",
    "con_test = confusion_matrix(y_test, pred_test)\n",
    "print(con_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       333\n",
      "           1       0.96      0.98      0.97       179\n",
      "\n",
      "    accuracy                           0.98       512\n",
      "   macro avg       0.97      0.98      0.97       512\n",
      "weighted avg       0.98      0.98      0.98       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_train = classification_report(y_train, pred_train)\n",
    "print(report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       111\n",
      "           1       0.94      1.00      0.97        60\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.97      0.98      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_test = classification_report(y_test, pred_test)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data2 = pd.read_csv(\"../Data/house_price.csv\", encoding=\"utf-8\")\n",
    "X = data2.iloc[:, 1:5]\n",
    "y = data2[[\"house_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 410)\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train)\n",
    "X_scaled_train = minmax.transform(X_train)\n",
    "X_scaled_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515068528461464"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import * \n",
    "from sklearn.neighbors import * \n",
    "from sklearn.ensemble import * \n",
    "\n",
    "estimators = [('lr', LinearRegression()), ('knn', KNeighborsRegressor())]\n",
    "model = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=10, random_state=410))\n",
    "model.fit(X_scaled_train, y_train)\n",
    "pred_train = model.predict(X_scaled_train)\n",
    "model.score(X_scaled_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46821087868350253"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_scaled_test)\n",
    "model.score(X_scaled_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean_squared_error in module sklearn.metrics._regression:\n",
      "\n",
      "mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "    Mean squared error regression loss.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "        Estimated target values.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        Defines aggregating of multiple output values.\n",
      "        Array-like value defines weights used to average errors.\n",
      "    \n",
      "        'raw_values' :\n",
      "            Returns a full set of errors in case of multioutput input.\n",
      "    \n",
      "        'uniform_average' :\n",
      "            Errors of all outputs are averaged with uniform weight.\n",
      "    \n",
      "    squared : bool, default=True\n",
      "        If True returns MSE value, if False returns RMSE value.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    loss : float or ndarray of floats\n",
      "        A non-negative floating point value (the best value is 0.0), or an\n",
      "        array of floating point values, one for each individual target.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import mean_squared_error\n",
      "    >>> y_true = [3, -0.5, 2, 7]\n",
      "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "    >>> mean_squared_error(y_true, y_pred)\n",
      "    0.375\n",
      "    >>> y_true = [3, -0.5, 2, 7]\n",
      "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "    >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "    0.612...\n",
      "    >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "    >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "    >>> mean_squared_error(y_true, y_pred)\n",
      "    0.708...\n",
      "    >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "    0.822...\n",
      "    >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "    array([0.41666667, 1.        ])\n",
      "    >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "    0.825...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 RMSE :  63534.97154496316\n",
      "테스트 데이터 RMSE :  70915.58690460226\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "MSE_train = mean_squared_error(y_train, pred_train)\n",
    "MSE_test = mean_squared_error(y_test, pred_test)\n",
    "\n",
    "print(\"학습 데이터 RMSE : \", np.sqrt(MSE_train))\n",
    "print(\"테스트 데이터 RMSE : \", np.sqrt(MSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e56095587f6faaee8b6fe269bd4758f04d4dcfca17a97e0204e5c06e32115c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
