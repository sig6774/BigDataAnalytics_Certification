{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브 베이즈 \n",
    "- 사건 B가 주어졌을 때 사건 A가 일어날 확률인 조건부 확률과 베이즈 정리를 이용한 알고리즘\n",
    "- 나이브 : 예측에 사용되는 X가 상호독립적이라는 가정에서 확률 계산을 단순화 하기 위한 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Cell_Size</th>\n",
       "      <th>Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>776715</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>841769</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>888820</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>897471</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  Clump_Thickness  Cell_Size  Cell_Shape  Marginal_Adhesion  \\\n",
       "0    1000025                5          1           1                  1   \n",
       "1    1002945                5          4           4                  5   \n",
       "2    1015425                3          1           1                  1   \n",
       "3    1016277                6          8           8                  1   \n",
       "4    1017023                4          1           1                  3   \n",
       "..       ...              ...        ...         ...                ...   \n",
       "678   776715                3          1           1                  1   \n",
       "679   841769                2          1           1                  1   \n",
       "680   888820                5         10          10                  3   \n",
       "681   897471                4          8           6                  4   \n",
       "682   897471                4          8           8                  5   \n",
       "\n",
       "     Single_Epithelial_Cell_Size  Bare_Nuclei  Bland_Chromatin  \\\n",
       "0                              2            1                3   \n",
       "1                              7           10                3   \n",
       "2                              2            2                3   \n",
       "3                              3            4                3   \n",
       "4                              2            1                3   \n",
       "..                           ...          ...              ...   \n",
       "678                            3            2                1   \n",
       "679                            2            1                1   \n",
       "680                            7            3                8   \n",
       "681                            3            4               10   \n",
       "682                            4            5               10   \n",
       "\n",
       "     Normal_Nucleoli  Mitoses  Class  \n",
       "0                  1        1      0  \n",
       "1                  2        1      0  \n",
       "2                  1        1      0  \n",
       "3                  7        1      0  \n",
       "4                  1        1      0  \n",
       "..               ...      ...    ...  \n",
       "678                1        1      0  \n",
       "679                1        1      0  \n",
       "680               10        2      1  \n",
       "681                6        1      1  \n",
       "682                4        1      1  \n",
       "\n",
       "[683 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../Data/breast-cancer-wisconsin.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:10]\n",
    "y = data[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 9)\n",
      "(512, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import *\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=410)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import *\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale_train = minmax.transform(X_train)\n",
    "X_scale_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCMeta',\n",
       " 'BaseEstimator',\n",
       " 'BernoulliNB',\n",
       " 'CategoricalNB',\n",
       " 'ClassifierMixin',\n",
       " 'ComplementNB',\n",
       " 'GaussianNB',\n",
       " 'LabelBinarizer',\n",
       " 'MultinomialNB',\n",
       " '_ALPHA_MIN',\n",
       " '_BaseDiscreteNB',\n",
       " '_BaseNB',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_check_partial_fit_first_call',\n",
       " '_check_sample_weight',\n",
       " 'abstractmethod',\n",
       " 'binarize',\n",
       " 'check_is_fitted',\n",
       " 'check_non_negative',\n",
       " 'deprecated',\n",
       " 'label_binarize',\n",
       " 'logsumexp',\n",
       " 'np',\n",
       " 'safe_sparse_dot',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn.naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sig6774/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.962890625"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB() \n",
    "model.fit(X_scale_train, y_train)\n",
    "pred_train = model.predict(X_scale_train)\n",
    "model.score(X_scale_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[319  14]\n",
      " [  5 174]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import * \n",
    "con_train = confusion_matrix(y_train, pred_train)\n",
    "print(con_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       333\n",
      "           1       0.93      0.97      0.95       179\n",
      "\n",
      "    accuracy                           0.96       512\n",
      "   macro avg       0.96      0.97      0.96       512\n",
      "weighted avg       0.96      0.96      0.96       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_train = classification_report(y_train, pred_train)\n",
    "print(report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_scale_test)\n",
    "model.score(X_scale_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106   5]\n",
      " [  0  60]]\n"
     ]
    }
   ],
   "source": [
    "con_test = confusion_matrix(y_test, pred_test)\n",
    "print(con_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       111\n",
      "           1       0.92      1.00      0.96        60\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.96      0.98      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_test = classification_report(y_test, pred_test)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(_BaseNB)\n",
      " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      " |  \n",
      " |  Gaussian Naive Bayes (GaussianNB).\n",
      " |  \n",
      " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like of shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, default=1e-9\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      class labels known to the classifier.\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      " |      Variance of each feature per class.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
      " |         Use `var_` instead.\n",
      " |  \n",
      " |  var_ : ndarray of shape (n_classes, n_features)\n",
      " |      Variance of each feature per class.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  theta_ : ndarray of shape (n_classes, n_features)\n",
      " |      mean of each feature per class.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
      " |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
      " |  ComplementNB : Complement Naive Bayes classifier.\n",
      " |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes,), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  sigma_\n",
      " |      DEPRECATED: Attribute `sigma_` was deprecated in 1.0 and will be removed in1.2. Use `var_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GaussianNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = {\"var_smoothing\" : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "gridSearch = GridSearchCV(GaussianNB(), paramGrid, cv = 5)\n",
    "gridSearch.fit(X_scale_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param : {'var_smoothing': 0}\n",
      "Best score : 0.9590138968208644\n",
      "Test score : 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "print(\"Best param : {}\".format(gridSearch.best_params_))\n",
    "print(\"Best score : {}\".format(gridSearch.best_score_))\n",
    "print(\"Test score : {}\".format(gridSearch.score(X_scale_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GaussianNB(), n_iter=100,\n",
       "                   param_distributions={'var_smoothing': <scipy.stats._distn_infrastructure.rv_frozen object at 0x161590370>})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint \n",
    "param_dist = {\"var_smoothing\" : randint(low = 0, high = 20)}\n",
    "randSearch = RandomizedSearchCV(GaussianNB(), param_dist, n_iter = 100, cv = 5)\n",
    "randSearch.fit(X_scale_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param : {'var_smoothing': 0}\n",
      "Best score : 0.9590138968208644\n",
      "Test score : 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Param : {}\".format(randSearch.best_params_))\n",
    "print(\"Best score : {}\".format(randSearch.best_score_))\n",
    "print(\"Test score : {}\".format(randSearch.score(X_scale_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data2 = pd.read_csv(\"../Data/house_price.csv\", encoding=\"utf-8\")\n",
    "X = data2.iloc[:, 1:5]\n",
    "y = data2[['house_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import * \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=410)\n",
    "\n",
    "from sklearn.preprocessing import * \n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train, y_train)\n",
    "X_scale_train = minmax.transform(X_train)\n",
    "X_scale_test = minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5465590994761103"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import * \n",
    "model = BayesianRidge()\n",
    "model.fit(X_scale_train, y_train)\n",
    "pred_train = model.predict(X_scale_train)\n",
    "model.score(X_scale_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5584432407220848"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(X_scale_test)\n",
    "model.score(X_scale_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 RMSE :  63884.46755516432\n",
      "테스트 데이터 RMSE :  64619.74586886851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import * \n",
    "MSE_train = mean_squared_error(y_train, pred_train)\n",
    "MSE_test = mean_squared_error(y_test, pred_test)\n",
    "\n",
    "RMSE_train = np.sqrt(MSE_train)\n",
    "RMSE_test = np.sqrt(MSE_test)\n",
    "print(\"학습 데이터 RMSE : \", RMSE_train)\n",
    "print(\"테스트 데이터 RMSE : \", RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BayesianRidge in module sklearn.linear_model._bayes:\n",
      "\n",
      "class BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      " |  BayesianRidge(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      " |  \n",
      " |  Bayesian ridge regression.\n",
      " |  \n",
      " |  Fit a Bayesian ridge model. See the Notes section for details on this\n",
      " |  implementation and the optimization of the regularization parameters\n",
      " |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_iter : int, default=300\n",
      " |      Maximum number of iterations. Should be greater than or equal to 1.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Stop the algorithm if w has converged.\n",
      " |  \n",
      " |  alpha_1 : float, default=1e-6\n",
      " |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      " |      over the alpha parameter.\n",
      " |  \n",
      " |  alpha_2 : float, default=1e-6\n",
      " |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      " |      Gamma distribution prior over the alpha parameter.\n",
      " |  \n",
      " |  lambda_1 : float, default=1e-6\n",
      " |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      " |      over the lambda parameter.\n",
      " |  \n",
      " |  lambda_2 : float, default=1e-6\n",
      " |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      " |      Gamma distribution prior over the lambda parameter.\n",
      " |  \n",
      " |  alpha_init : float, default=None\n",
      " |      Initial value for alpha (precision of the noise).\n",
      " |      If not set, alpha_init is 1/Var(y).\n",
      " |  \n",
      " |          .. versionadded:: 0.22\n",
      " |  \n",
      " |  lambda_init : float, default=None\n",
      " |      Initial value for lambda (precision of the weights).\n",
      " |      If not set, lambda_init is 1.\n",
      " |  \n",
      " |          .. versionadded:: 0.22\n",
      " |  \n",
      " |  compute_score : bool, default=False\n",
      " |      If True, compute the log marginal likelihood at each iteration of the\n",
      " |      optimization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model.\n",
      " |      The intercept is not treated as a probabilistic parameter\n",
      " |      and thus has no associated variance. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      " |          1.2.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Verbose mode when fitting the model.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array-like of shape (n_features,)\n",
      " |      Coefficients of the regression model (mean of distribution)\n",
      " |  \n",
      " |  intercept_ : float\n",
      " |      Independent term in decision function. Set to 0.0 if\n",
      " |      ``fit_intercept = False``.\n",
      " |  \n",
      " |  alpha_ : float\n",
      " |     Estimated precision of the noise.\n",
      " |  \n",
      " |  lambda_ : float\n",
      " |     Estimated precision of the weights.\n",
      " |  \n",
      " |  sigma_ : array-like of shape (n_features, n_features)\n",
      " |      Estimated variance-covariance matrix of the weights\n",
      " |  \n",
      " |  scores_ : array-like of shape (n_iter_+1,)\n",
      " |      If computed_score is True, value of the log marginal likelihood (to be\n",
      " |      maximized) at each iteration of the optimization. The array starts\n",
      " |      with the value of the log marginal likelihood obtained for the initial\n",
      " |      values of alpha and lambda and ends with the value obtained for the\n",
      " |      estimated alpha and lambda.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations to reach the stopping criterion.\n",
      " |  \n",
      " |  X_offset_ : float\n",
      " |      If `normalize=True`, offset subtracted for centering data to a\n",
      " |      zero mean.\n",
      " |  \n",
      " |  X_scale_ : float\n",
      " |      If `normalize=True`, parameter used to scale data to a unit\n",
      " |      standard deviation.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  ARDRegression : Bayesian ARD regression.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  There exist several strategies to perform Bayesian ridge regression. This\n",
      " |  implementation is based on the algorithm described in Appendix A of\n",
      " |  (Tipping, 2001) where updates of the regularization parameters are done as\n",
      " |  suggested in (MacKay, 1992). Note that according to A New\n",
      " |  View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
      " |  update rules do not guarantee that the marginal likelihood is increasing\n",
      " |  between two consecutive iterations of the optimization.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      " |  Vol. 4, No. 3, 1992.\n",
      " |  \n",
      " |  M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
      " |  Journal of Machine Learning Research, Vol. 1, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import linear_model\n",
      " |  >>> clf = linear_model.BayesianRidge()\n",
      " |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      " |  BayesianRidge()\n",
      " |  >>> clf.predict([[1, 1]])\n",
      " |  array([1.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BayesianRidge\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values. Will be cast to X's dtype if necessary.\n",
      " |      \n",
      " |      sample_weight : ndarray of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.20\n",
      " |             parameter *sample_weight* support to BayesianRidge.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  predict(self, X, return_std=False)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      In addition to the mean of the predictive distribution, also its\n",
      " |      standard deviation can be returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      return_std : bool, default=False\n",
      " |          Whether to return the standard deviation of posterior prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_mean : array-like of shape (n_samples,)\n",
      " |          Mean of predictive distribution of query points.\n",
      " |      \n",
      " |      y_std : array-like of shape (n_samples,)\n",
      " |          Standard deviation of predictive distribution of query points.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=BayesianRidge(),\n",
       "             param_grid={'alpha_1': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                     2, 3, 4],\n",
       "                         'lambda_1': [1e-06, 1e-05, 1e-05, 0.001, 0.01, 0.1, 1,\n",
       "                                      2, 3, 4]})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\"alpha_1\" : [1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1, 2, 3, 4],\n",
    "              \"lambda_1\" : [1e-06, 1e-05, 1e-05, 1e-03, 1e-02, 1e-01, 1, 2, 3, 4]}\n",
    "gridSearch = GridSearchCV(BayesianRidge(), param_grid, cv = 5)\n",
    "gridSearch.fit(X_scale_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param : {'alpha_1': 1e-06, 'lambda_1': 1e-05}\n",
      "Best Score : 0.545832731067396\n",
      "Test score : 0.5584432406809501\n"
     ]
    }
   ],
   "source": [
    "print(\"Best param : {}\".format(gridSearch.best_params_))\n",
    "print(\"Best Score : {}\".format(gridSearch.best_score_))\n",
    "print(\"Test score : {}\".format(gridSearch.score(X_scale_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=BayesianRidge(), n_iter=100,\n",
       "                   param_distributions={'alpha_1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x161657c10>,\n",
       "                                        'lambda_1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x16165cd90>})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint \n",
    "param_dist = {\"alpha_1\" : randint(low = 1e-06, high = 10),\n",
    "              \"lambda_1\" : randint(low=1e-06, high=10)}\n",
    "randSearch = RandomizedSearchCV(BayesianRidge(), param_dist, n_iter=100, cv = 5)\n",
    "randSearch.fit(X_scale_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param : {'alpha_1': 0, 'lambda_1': 0}\n",
      "Best score : 0.5458327310666804\n",
      "Test score : 0.558443240726654\n"
     ]
    }
   ],
   "source": [
    "print(\"Best param : {}\".format(randSearch.best_params_))\n",
    "print(\"Best score : {}\".format(randSearch.best_score_))\n",
    "print(\"Test score : {}\".format(randSearch.score(X_scale_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e56095587f6faaee8b6fe269bd4758f04d4dcfca17a97e0204e5c06e32115c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
